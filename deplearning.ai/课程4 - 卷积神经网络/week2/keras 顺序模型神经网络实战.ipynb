{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#查看数据集\" data-toc-modified-id=\"查看数据集-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>查看数据集</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Dense, Dropout, Activation  \n",
    "from keras.optimizers import SGD  \n",
    "from keras.datasets import mnist  \n",
    "import numpy \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "# 使用Keras自带的mnist工具读取数据（第一次需要联网）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM7UlEQVR4nO3df4wc9XnH8c+Hw9iSSSIbF2qMSwgYGpIKE53cJq4QDSl1SCWTP6jiStSRUI9EkJKKSkH0j/iPqnIr8kstjXqAi4kSU6oE4USkiWUhAaIQDtc1dp3ii+sax1c7lEYxUbDP9tM/bqgu5nb2vDOzs/bzfkmn3Z1nd+bR6D43u/udua8jQgDOfue03QCA/iDsQBKEHUiCsANJEHYgiXP7ubHzPDfmaX4/Nwmk8qZ+rmNx1DPVKoXd9ipJX5E0JOnBiFhf9vx5mq/f9A1VNgmgxAuxtWOt57fxtock3S/po5KulrTG9tW9rg9As6p8Zl8haTwi9kbEMUmPSlpdT1sA6lYl7EskvTrt8YFi2S+xPWJ7zPbYpI5W2ByAKqqEfaYvAd527m1EjEbEcEQMz9HcCpsDUEWVsB+QtHTa40skHazWDoCmVAn7i5KW2b7M9nmSPiFpcz1tAahbz0NvEXHc9p2SvqepobcNEbGrts4A1KrSOHtEPCnpyZp6AdAgTpcFkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiUqzuAJN2rv+g6X1H956f2n9is2f6li78tM/6KmnM1mlsNveJ+mIpBOSjkfEcB1NAahfHUf234mI12pYD4AG8ZkdSKJq2EPS922/ZHtkpifYHrE9ZntsUkcrbg5Ar6q+jV8ZEQdtXyhpi+0fRsTT058QEaOSRiXpnV4YFbcHoEeVjuwRcbC4PSzpcUkr6mgKQP16Drvt+bbf8dZ9STdK2llXYwDqVeVt/EWSHrf91nq+ERH/XEtXSGHogoWl9Qdu+fvS+kmVfyrc9vtf7li77kd/Vvrai+97rrR+Juo57BGxV9I1NfYCoEEMvQFJEHYgCcIOJEHYgSQIO5AEl7iiUZ5zXsfa+N1Xlb525bwtlbb9zJuLOtYufvaNSus+E3FkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdHo36xannH2q61f9votv9y3R91rL3r+ecb3fYg4sgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzo5Kht5Xfk36h//i2ca2vWbv75XWF2ze1bF2su5mzgAc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcbZUclH/vHF0vpnFuzped3jk0dL6xN/c0Vp/fwj+a5ZL9P1yG57g+3DtndOW7bQ9hbbe4rbBc22CaCq2byNf1jSqlOW3SNpa0Qsk7S1eAxggHUNe0Q8Len1UxavlrSxuL9R0s31tgWgbr1+QXdRRExIUnF7Yacn2h6xPWZ7bFLln8EANKfxb+MjYjQihiNieI7mNr05AB30GvZDthdLUnF7uL6WADSh17BvlrS2uL9W0hP1tAOgKV3H2W1vknS9pEW2D0j6vKT1kh6zfZuk/ZJuabJJDK67FoyX1qtcN/6x7/xpaX3ZY4yjn46uYY+INR1KN9TcC4AGcboskARhB5Ig7EAShB1IgrADSXCJK0q98uBwaX3I20vrJ+NEx1q3fwV91ed2ltYz/jvoKjiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLMnN/61a0vrD37oH0rrJ6J8tPuv/ud9HWu/+MN5pa89+fPXSus4PRzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnPcpM3ll+PPtplHP26ece6bMGl1U2bPtyxdsmrz3VZN+rEkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCc/Sy3/5PHS+vXz5vssobycfS/++llpfVL/2miY63zf5RHE7oe2W1vsH3Y9s5py9bZ/rHt7cXPTc22CaCq2byNf1jSqhmWfykilhc/T9bbFoC6dQ17RDwt6fU+9AKgQVW+oLvT9o7ibf6CTk+yPWJ7zPbYpI5W2ByAKnoN+1clXS5puaQJSV/o9MSIGI2I4YgYnqO5PW4OQFU9hT0iDkXEiYg4KekBSSvqbQtA3XoKu+3F0x5+XFL53LoAWtd1nN32JknXS1pk+4Ckz0u63vZySSFpn6Tbm2sR3exf96GOtR3Xfbn0tSc1VGnb3139gdL6ifH/rLR+1Kdr2CNizQyLH2qgFwAN4nRZIAnCDiRB2IEkCDuQBGEHkuAS1zPA0FVXlNZvv6XzdUhzXD60djTKL3G95qlPl9avGP/X0joGB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYBcM7yq0vrH/vGM6X1kXft63nbv/GdPymtX/mpH/S8bgwWjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7APgf9//ztJ6lXH0bi79djS2bgwWjuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7APgzYXN/c1du+8jpfX5L+0vrR+vsxm0qutvme2ltp+yvdv2Ltt3FcsX2t5ie09xu6D5dgH0ajaHlOOS7o6I90r6LUl32L5a0j2StkbEMklbi8cABlTXsEfERERsK+4fkbRb0hJJqyVtLJ62UdLNDfUIoAan9WHR9rslXSvpBUkXRcSENPUHQdKFHV4zYnvM9tikjlZsF0CvZh122+dL+qakz0bEz2b7uogYjYjhiBieo7m99AigBrMKu+05mgr61yPiW8XiQ7YXF/XFkg430yKAOnQderNtSQ9J2h0RX5xW2ixpraT1xe0TjXSYwLrPPNLYul95+NdL6xf89780tm0MltmMs6+UdKukl21vL5bdq6mQP2b7Nkn7Jd3SSIcAatE17BHxrCR3KN9QbzsAmsLpskAShB1IgrADSRB2IAnCDiTBJa59EB+8prS+9Nxu0yIPlVbf++gdHWuXP/h8l3UjC47sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+x9MLFyfmn9Ped2+4fN5ePs5xzrdFGipGBKZkzhyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDO3gcX3/dcaf2Z2xeV1pec+9PS+q99j2m10B1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYjbzsy+V9IikX5V0UtJoRHzF9jpJfyzpJ8VT742IJ5tq9Gx2/7IrK71+SNtq6gRns9mcVHNc0t0Rsc32OyS9ZHtLUftSRNzXXHsA6jKb+dknJE0U94/Y3i1pSdONAajXaX1mt/1uSddKeqFYdKftHbY32F7Q4TUjtsdsj02K0zqBtsw67LbPl/RNSZ+NiJ9J+qqkyyUt19SR/wszvS4iRiNiOCKG52hu9Y4B9GRWYbc9R1NB/3pEfEuSIuJQRJyIiJOSHpC0ork2AVTVNey2LekhSbsj4ovTli+e9rSPS9pZf3sA6jKbb+NXSrpV0su2txfL7pW0xvZySSFpn6TbG+gPQE1m8238s5Jm+sfkjKkDZxDOoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiThiOjfxuyfSPqvaYsWSXqtbw2cnkHtbVD7kuitV3X2dmlE/MpMhb6G/W0bt8ciYri1BkoMam+D2pdEb73qV2+8jQeSIOxAEm2HfbTl7ZcZ1N4GtS+J3nrVl95a/cwOoH/aPrID6BPCDiTRSthtr7L9H7bHbd/TRg+d2N5n+2Xb222PtdzLBtuHbe+ctmyh7S229xS3M86x11Jv62z/uNh3223f1FJvS20/ZXu37V227yqWt7rvSvrqy37r+2d220OSXpH0u5IOSHpR0pqI+Pe+NtKB7X2ShiOi9RMwbF8n6Q1Jj0TE+4tlfy3p9YhYX/yhXBARnxuQ3tZJeqPtabyL2YoWT59mXNLNkj6pFvddSV9/oD7stzaO7CskjUfE3og4JulRSatb6GPgRcTTkl4/ZfFqSRuL+xs19cvSdx16GwgRMRER24r7RyS9Nc14q/uupK++aCPsSyS9Ou3xAQ3WfO8h6fu2X7I90nYzM7goIiakqV8eSRe23M+puk7j3U+nTDM+MPuul+nPq2oj7DNNJTVI438rI+IDkj4q6Y7i7SpmZ1bTePfLDNOMD4Repz+vqo2wH5C0dNrjSyQdbKGPGUXEweL2sKTHNXhTUR96awbd4vZwy/38v0GaxnumacY1APuuzenP2wj7i5KW2b7M9nmSPiFpcwt9vI3t+cUXJ7I9X9KNGrypqDdLWlvcXyvpiRZ7+SWDMo13p2nG1fK+a33684jo+4+kmzT1jfyPJP15Gz106Os9kv6t+NnVdm+SNmnqbd2kpt4R3SbpAklbJe0pbhcOUG9fk/SypB2aCtbilnr7bU19NNwhaXvxc1Pb+66kr77sN06XBZLgDDogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/AO2JzMC+7ms4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[23])\n",
    "print(X_train.shape)\n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据大小： (60000, 28, 28)\n",
      "测试集数据大小： (10000, 28, 28)\n",
      "训练集标签 (60000, 10)\n",
      "测试集标签 (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集数据大小：\", X_train.shape)\n",
    "print(\"测试集数据大小：\", X_test.shape)\n",
    "print(\"训练集标签\", Y_train.shape)\n",
    "print(\"测试集标签\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第一步：选择模型\n",
    "'''\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   第二步：构建网络层\n",
    "'''\n",
    "model.add(Dense(500,input_shape=(784,))) # 输入层，28*28=784  \n",
    "model.add(Activation('tanh')) # 激活函数是tanh  \n",
    "model.add(Dropout(0.5)) # 采用50%的dropout\n",
    "\n",
    "model.add(Dense(500)) # 隐藏层节点500个  \n",
    "model.add(Activation('tanh'))  \n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10)) # 输出结果是10个类别，所以维度是10  \n",
    "model.add(Activation('softmax')) # 最后一层用softmax作为激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5010      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 648,010\n",
      "Trainable params: 648,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   第三步：编译\n",
    "'''\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) # 优化函数，设定学习率（lr）等参数  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd) # 使用交叉熵作为loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/50\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 1.1929 - val_loss: 0.5454\n",
      "Epoch 2/50\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.9417 - val_loss: 0.5062\n",
      "Epoch 3/50\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 0.8633 - val_loss: 0.4640\n",
      "Epoch 4/50\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8091 - val_loss: 0.4521\n",
      "Epoch 5/50\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.7655 - val_loss: 0.4087\n",
      "Epoch 6/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.7297 - val_loss: 0.3965\n",
      "Epoch 7/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.6909 - val_loss: 0.3932\n",
      "Epoch 8/50\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.6823 - val_loss: 0.3827\n",
      "Epoch 9/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.7090 - val_loss: 0.3964\n",
      "Epoch 10/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.6818 - val_loss: 0.3820\n",
      "Epoch 11/50\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.6446 - val_loss: 0.3666\n",
      "Epoch 12/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.6263 - val_loss: 0.3549\n",
      "Epoch 13/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.6066 - val_loss: 0.3488\n",
      "Epoch 14/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.6267 - val_loss: 0.3555\n",
      "Epoch 15/50\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.6108 - val_loss: 0.3334\n",
      "Epoch 16/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.5934 - val_loss: 0.3364\n",
      "Epoch 17/50\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.5782 - val_loss: 0.3383\n",
      "Epoch 18/50\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.5692 - val_loss: 0.3250\n",
      "Epoch 19/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.5789 - val_loss: 0.3269\n",
      "Epoch 20/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.5730 - val_loss: 0.3259\n",
      "Epoch 21/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.5642 - val_loss: 0.3174\n",
      "Epoch 22/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.5618 - val_loss: 0.3163\n",
      "Epoch 23/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.5671 - val_loss: 0.3195\n",
      "Epoch 24/50\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.5396 - val_loss: 0.3136\n",
      "Epoch 25/50\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.5337 - val_loss: 0.3078\n",
      "Epoch 26/50\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.5325 - val_loss: 0.2996\n",
      "Epoch 27/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.5108 - val_loss: 0.2954\n",
      "Epoch 28/50\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.5167 - val_loss: 0.2892\n",
      "Epoch 29/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.5004 - val_loss: 0.2940\n",
      "Epoch 30/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.4852 - val_loss: 0.2831\n",
      "Epoch 31/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.4829 - val_loss: 0.2815\n",
      "Epoch 32/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.4659 - val_loss: 0.2791\n",
      "Epoch 33/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.4501 - val_loss: 0.2773\n",
      "Epoch 34/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.4708 - val_loss: 0.2769\n",
      "Epoch 35/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.4776 - val_loss: 0.2742\n",
      "Epoch 36/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.4667 - val_loss: 0.2735\n",
      "Epoch 37/50\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.4790 - val_loss: 0.2801\n",
      "Epoch 38/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.4912 - val_loss: 0.2833\n",
      "Epoch 39/50\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.4663 - val_loss: 0.2715\n",
      "Epoch 40/50\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.4687 - val_loss: 0.2735\n",
      "Epoch 41/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.4741 - val_loss: 0.2705\n",
      "Epoch 42/50\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.4638 - val_loss: 0.2732\n",
      "Epoch 43/50\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.4653 - val_loss: 0.2594\n",
      "Epoch 44/50\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.4534 - val_loss: 0.2543\n",
      "Epoch 45/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.4532 - val_loss: 0.2491\n",
      "Epoch 46/50\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.4347 - val_loss: 0.2492\n",
      "Epoch 47/50\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.4498 - val_loss: 0.2525\n",
      "Epoch 48/50\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.4504 - val_loss: 0.2491\n",
      "Epoch 49/50\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.4471 - val_loss: 0.2503\n",
      "Epoch 50/50\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.4247 - val_loss: 0.2425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23831116840243338"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "   第四步：训练\n",
    "   .fit的一些参数\n",
    "   batch_size：对总的样本数进行分组，每组包含的样本数量\n",
    "   epochs ：训练次数\n",
    "   shuffle：是否把数据随机打乱之后再进行训练\n",
    "   validation_split：拿出百分之多少用来做交叉验证\n",
    "   verbose：屏显模式 0：不输出  1：输出进度  2：输出每次的训练结果\n",
    "'''\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # 使用Keras自带的mnist工具读取数据（第一次需要联网）\n",
    "# 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维  \n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]) \n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2])  \n",
    "Y_train = (numpy.arange(10) == y_train[:, None]).astype(int) \n",
    "Y_test = (numpy.arange(10) == y_test[:, None]).astype(int)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    batch_size=200,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    validation_split=0.3)\n",
    "model.evaluate(X_test, Y_test, batch_size=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n",
      "\n",
      "The test loss is 0.238311\n",
      "\n",
      "The accuracy of the model is 0.929500\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    第五步：输出\n",
    "'''\n",
    "print(\"test set\")\n",
    "scores = model.evaluate(X_test,Y_test,batch_size=200,verbose=0)\n",
    "print(\"\")\n",
    "print(\"The test loss is %f\" % scores)\n",
    "result = model.predict(X_test,batch_size=200,verbose=0)\n",
    "\n",
    "result_max = numpy.argmax(result, axis = 1)\n",
    "test_max = numpy.argmax(Y_test, axis = 1)\n",
    "\n",
    "result_bool = numpy.equal(result_max, test_max)\n",
    "true_num = numpy.sum(result_bool)\n",
    "print(\"\")\n",
    "print(\"The accuracy of the model is %f\" % (true_num/len(result_bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tf1]",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
