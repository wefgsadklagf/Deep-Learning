{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "import resnets_utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    实现图3的恒等块\n",
    "    \n",
    "    参数：\n",
    "        X - 输入的tensor类型的数据，维度为( m, n_H_prev, n_W_prev, n_H_prev )\n",
    "        f - 整数，指定主路径中间的CONV窗口的维度\n",
    "        filters - 整数列表，定义了主路径每层的卷积层的过滤器数量\n",
    "        stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。\n",
    "        block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。\n",
    "        \n",
    "    返回：\n",
    "        X - 恒等块的输出，tensor类型，维度为(n_H, n_W, n_C)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #定义命名规则\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "    \n",
    "    #获取过滤器\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    #保存输入数据，将会用于为主路径添加捷径\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #主路径的第一部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(1,1) ,padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    #主路径的第二部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f),strides=(1,1), padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    \n",
    "    #主路径的第三部分\n",
    "    ##卷积层\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    ##归一化\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "    ##没有ReLU激活函数\n",
    "    \n",
    "    #最后一步：\n",
    "    ##将捷径与输入加在一起\n",
    "    X = Add()([X,X_shortcut])\n",
    "    ##使用ReLU激活函数\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    \"\"\"\n",
    "    实现图5的卷积块\n",
    "    \n",
    "    参数：\n",
    "        X - 输入的tensor类型的变量，维度为( m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        f - 整数，指定主路径中间的CONV窗口的维度\n",
    "        filters - 整数列表，定义了主路径每层的卷积层的过滤器数量\n",
    "        stage - 整数，根据每层的位置来命名每一层，与block参数一起使用。\n",
    "        block - 字符串，据每层的位置来命名每一层，与stage参数一起使用。\n",
    "        s - 整数，指定要使用的步幅\n",
    "    \n",
    "    返回：\n",
    "        X - 卷积块的输出，tensor类型，维度为(n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    #定义命名规则\n",
    "    conv_name_base = \"res\" + str(stage) + block + \"_branch\"\n",
    "    bn_name_base   = \"bn\"  + str(stage) + block + \"_branch\"\n",
    "    \n",
    "    #获取过滤器数量\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    #保存输入数据\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #主路径\n",
    "    ##主路径第一部分\n",
    "    X = Conv2D(filters=F1, kernel_size=(1,1), strides=(s,s), padding=\"valid\",\n",
    "               name=conv_name_base+\"2a\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2a\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    ##主路径第二部分\n",
    "    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(1,1), padding=\"same\",\n",
    "               name=conv_name_base+\"2b\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2b\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    ##主路径第三部分\n",
    "    X = Conv2D(filters=F3, kernel_size=(1,1), strides=(1,1), padding=\"valid\",\n",
    "               name=conv_name_base+\"2c\", kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3,name=bn_name_base+\"2c\")(X)\n",
    "    \n",
    "    #捷径\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1,1), strides=(s,s), padding=\"valid\",\n",
    "               name=conv_name_base+\"1\", kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3,name=bn_name_base+\"1\")(X_shortcut)\n",
    "    \n",
    "    #最后一步\n",
    "    X = Add()([X,X_shortcut])\n",
    "    X = Activation(\"relu\")(X)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(64,64,3),classes=6):\n",
    "    \"\"\"\n",
    "    实现ResNet50\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "    \n",
    "    参数：\n",
    "        input_shape - 图像数据集的维度\n",
    "        classes - 整数，分类数\n",
    "        \n",
    "    返回：\n",
    "        model - Keras框架的模型\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #定义tensor类型的输入数据\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    #0填充\n",
    "    X = ZeroPadding2D((3,3))(X_input)\n",
    "    \n",
    "    #stage1\n",
    "    X = Conv2D(filters=64, kernel_size=(7,7), strides=(2,2), name=\"conv1\",\n",
    "               kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=\"bn_conv1\")(X)\n",
    "    X = Activation(\"relu\")(X)\n",
    "    X = MaxPooling2D(pool_size=(3,3), strides=(2,2))(X)\n",
    "    \n",
    "    #stage2\n",
    "    X = convolutional_block(X, f=3, filters=[64,64,256], stage=2, block=\"a\", s=1)\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[64,64,256], stage=2, block=\"c\")\n",
    "    \n",
    "    #stage3\n",
    "    X = convolutional_block(X, f=3, filters=[128,128,512], stage=3, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[128,128,512], stage=3, block=\"d\")\n",
    "    \n",
    "    #stage4\n",
    "    X = convolutional_block(X, f=3, filters=[256,256,1024], stage=4, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"c\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"d\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"e\")\n",
    "    X = identity_block(X, f=3, filters=[256,256,1024], stage=4, block=\"f\")\n",
    "    \n",
    "    #stage5\n",
    "    X = convolutional_block(X, f=3, filters=[512,512,2048], stage=5, block=\"a\", s=2)\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"b\")\n",
    "    X = identity_block(X, f=3, filters=[512,512,2048], stage=5, block=\"c\")\n",
    "    \n",
    "    #均值池化层\n",
    "    X = AveragePooling2D(pool_size=(2,2),padding=\"same\")(X)\n",
    "    \n",
    "    #输出层\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation=\"softmax\", name=\"fc\"+str(classes),\n",
    "              kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    #创建模型\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"ResNet50\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape=(64,64,3),classes=6)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = resnets_utils.load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig / 255.\n",
    "X_test = X_test_orig / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = resnets_utils.convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = resnets_utils.convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1080/1080 [==============================] - 266s 246ms/step - loss: 0.2988 - acc: 0.9185\n",
      "Epoch 2/120\n",
      "1080/1080 [==============================] - 266s 246ms/step - loss: 0.3275 - acc: 0.9083\n",
      "Epoch 3/120\n",
      "1080/1080 [==============================] - 266s 246ms/step - loss: 0.2836 - acc: 0.9315\n",
      "Epoch 4/120\n",
      "1080/1080 [==============================] - 268s 248ms/step - loss: 0.2031 - acc: 0.9389\n",
      "Epoch 5/120\n",
      "1080/1080 [==============================] - 268s 248ms/step - loss: 0.1484 - acc: 0.9509\n",
      "Epoch 6/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.2212 - acc: 0.9454\n",
      "Epoch 7/120\n",
      "1080/1080 [==============================] - 266s 247ms/step - loss: 0.1667 - acc: 0.9481\n",
      "Epoch 8/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0537 - acc: 0.9870\n",
      "Epoch 9/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0356 - acc: 0.9889\n",
      "Epoch 10/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0757 - acc: 0.9833\n",
      "Epoch 11/120\n",
      "1080/1080 [==============================] - 266s 247ms/step - loss: 0.0445 - acc: 0.9870\n",
      "Epoch 12/120\n",
      "1080/1080 [==============================] - 269s 249ms/step - loss: 0.1044 - acc: 0.9685\n",
      "Epoch 13/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0401 - acc: 0.9889\n",
      "Epoch 14/120\n",
      "1080/1080 [==============================] - 266s 247ms/step - loss: 0.1553 - acc: 0.9694\n",
      "Epoch 15/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0933 - acc: 0.9694\n",
      "Epoch 16/120\n",
      "1080/1080 [==============================] - 267s 247ms/step - loss: 0.0490 - acc: 0.9843\n",
      "Epoch 17/120\n",
      "1080/1080 [==============================] - 258s 239ms/step - loss: 0.0781 - acc: 0.9787\n",
      "Epoch 18/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.1861 - acc: 0.9546\n",
      "Epoch 19/120\n",
      "1080/1080 [==============================] - 247s 228ms/step - loss: 0.4443 - acc: 0.9083\n",
      "Epoch 20/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.2328 - acc: 0.9324\n",
      "Epoch 21/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0444 - acc: 0.9870\n",
      "Epoch 22/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0999 - acc: 0.9685\n",
      "Epoch 23/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0355 - acc: 0.9907\n",
      "Epoch 24/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0100 - acc: 0.9972\n",
      "Epoch 25/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 26/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 9.6069e-04 - acc: 1.0000\n",
      "Epoch 27/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0016 - acc: 0.9991\n",
      "Epoch 28/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0076 - acc: 0.9981\n",
      "Epoch 29/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0628 - acc: 0.9852\n",
      "Epoch 30/120\n",
      "1080/1080 [==============================] - 247s 229ms/step - loss: 0.0435 - acc: 0.9898\n",
      "Epoch 31/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.1134 - acc: 0.9685\n",
      "Epoch 32/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.2033 - acc: 0.9481\n",
      "Epoch 33/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0816 - acc: 0.9722\n",
      "Epoch 34/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0188 - acc: 0.9963\n",
      "Epoch 35/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0194 - acc: 0.9935\n",
      "Epoch 36/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 37/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0172 - acc: 0.9991\n",
      "Epoch 38/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0344 - acc: 0.9935\n",
      "Epoch 39/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0372 - acc: 0.9898\n",
      "Epoch 40/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0454 - acc: 0.9889\n",
      "Epoch 41/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0487 - acc: 0.9861\n",
      "Epoch 42/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0345 - acc: 0.9880\n",
      "Epoch 43/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0432 - acc: 0.9880\n",
      "Epoch 44/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0513 - acc: 0.9889\n",
      "Epoch 45/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0085 - acc: 0.9954\n",
      "Epoch 46/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0108 - acc: 0.9963\n",
      "Epoch 47/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0067 - acc: 0.9972\n",
      "Epoch 48/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0270 - acc: 0.9935\n",
      "Epoch 49/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0063 - acc: 0.9991\n",
      "Epoch 50/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 51/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 7.7016e-04 - acc: 1.0000\n",
      "Epoch 52/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 3.7612e-04 - acc: 1.0000\n",
      "Epoch 53/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 4.5781e-04 - acc: 1.0000\n",
      "Epoch 54/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 1.7058e-04 - acc: 1.0000\n",
      "Epoch 55/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 2.3785e-04 - acc: 1.0000\n",
      "Epoch 56/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 1.0646e-04 - acc: 1.0000\n",
      "Epoch 57/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 1.5818e-04 - acc: 1.0000\n",
      "Epoch 58/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 3.3368e-04 - acc: 1.0000\n",
      "Epoch 59/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0372 - acc: 0.9917\n",
      "Epoch 60/120\n",
      "1080/1080 [==============================] - 246s 227ms/step - loss: 0.0589 - acc: 0.9852\n",
      "Epoch 61/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0102 - acc: 0.9972\n",
      "Epoch 62/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 63/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 64/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0092 - acc: 0.9981\n",
      "Epoch 65/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0068 - acc: 0.9972\n",
      "Epoch 66/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0177 - acc: 0.9944\n",
      "Epoch 67/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0856 - acc: 0.9722\n",
      "Epoch 68/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0774 - acc: 0.9787\n",
      "Epoch 69/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.0700 - acc: 0.9769\n",
      "Epoch 70/120\n",
      "1080/1080 [==============================] - 247s 229ms/step - loss: 0.0201 - acc: 0.9926\n",
      "Epoch 71/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0190 - acc: 0.9926\n",
      "Epoch 72/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0133 - acc: 0.9963\n",
      "Epoch 73/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0076 - acc: 0.9972\n",
      "Epoch 74/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.0131 - acc: 0.9954\n",
      "Epoch 75/120\n",
      "1080/1080 [==============================] - 250s 231ms/step - loss: 0.0044 - acc: 0.9972\n",
      "Epoch 76/120\n",
      "1080/1080 [==============================] - 248s 229ms/step - loss: 0.0111 - acc: 0.9972\n",
      "Epoch 77/120\n",
      "1080/1080 [==============================] - 249s 230ms/step - loss: 0.0334 - acc: 0.9861\n",
      "Epoch 78/120\n",
      "1080/1080 [==============================] - 248s 229ms/step - loss: 0.0841 - acc: 0.9815\n",
      "Epoch 79/120\n",
      "1080/1080 [==============================] - 247s 229ms/step - loss: 0.1220 - acc: 0.9741\n",
      "Epoch 80/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080/1080 [==============================] - 247s 229ms/step - loss: 0.7327 - acc: 0.8120\n",
      "Epoch 81/120\n",
      "1080/1080 [==============================] - 247s 229ms/step - loss: 0.2174 - acc: 0.9435\n",
      "Epoch 82/120\n",
      "1080/1080 [==============================] - 249s 230ms/step - loss: 0.0664 - acc: 0.9778\n",
      "Epoch 83/120\n",
      "1080/1080 [==============================] - 249s 230ms/step - loss: 0.0548 - acc: 0.9843\n",
      "Epoch 84/120\n",
      "1080/1080 [==============================] - 251s 232ms/step - loss: 0.0594 - acc: 0.9870\n",
      "Epoch 85/120\n",
      "1080/1080 [==============================] - 250s 232ms/step - loss: 0.0520 - acc: 0.9824\n",
      "Epoch 86/120\n",
      "1080/1080 [==============================] - 247s 228ms/step - loss: 0.0327 - acc: 0.9889\n",
      "Epoch 87/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0612 - acc: 0.9880\n",
      "Epoch 88/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.2552 - acc: 0.9444\n",
      "Epoch 89/120\n",
      "1080/1080 [==============================] - 246s 228ms/step - loss: 0.4547 - acc: 0.8796\n",
      "Epoch 90/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.1579 - acc: 0.9602\n",
      "Epoch 91/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.1185 - acc: 0.9611\n",
      "Epoch 92/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.0353 - acc: 0.9889\n",
      "Epoch 93/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0548 - acc: 0.9907\n",
      "Epoch 94/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.2051 - acc: 0.9389\n",
      "Epoch 95/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.1006 - acc: 0.9667\n",
      "Epoch 96/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0280 - acc: 0.9926\n",
      "Epoch 97/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0532 - acc: 0.9815\n",
      "Epoch 98/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0837 - acc: 0.9778\n",
      "Epoch 99/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0445 - acc: 0.9889\n",
      "Epoch 100/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 0.0041 - acc: 0.9991\n",
      "Epoch 101/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0471 - acc: 0.9972\n",
      "Epoch 102/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 103/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 9.0510e-04 - acc: 1.0000\n",
      "Epoch 104/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 6.2876e-04 - acc: 1.0000\n",
      "Epoch 105/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 5.5843e-04 - acc: 1.0000\n",
      "Epoch 106/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 6.4814e-04 - acc: 1.0000\n",
      "Epoch 107/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 2.0847e-04 - acc: 1.0000\n",
      "Epoch 108/120\n",
      "1080/1080 [==============================] - 245s 226ms/step - loss: 2.6211e-04 - acc: 1.0000\n",
      "Epoch 109/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0018 - acc: 0.9991\n",
      "Epoch 110/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0242 - acc: 0.9926\n",
      "Epoch 111/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0152 - acc: 0.9944\n",
      "Epoch 112/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0077 - acc: 0.9972\n",
      "Epoch 113/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0282 - acc: 0.9963\n",
      "Epoch 114/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0216 - acc: 0.9944\n",
      "Epoch 115/120\n",
      "1080/1080 [==============================] - 245s 227ms/step - loss: 0.0173 - acc: 0.9954\n",
      "Epoch 116/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 117/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 0.0156 - acc: 0.9991\n",
      "Epoch 118/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 4.1823e-04 - acc: 1.0000\n",
      "Epoch 119/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 2.9259e-04 - acc: 1.0000\n",
      "Epoch 120/120\n",
      "1080/1080 [==============================] - 244s 226ms/step - loss: 1.6729e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e56b3fe48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=120,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 24s 204ms/step\n",
      "误差值 = 0.02622312183181445\n",
      "准确率 = 0.9833333293596903\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test,Y_test)\n",
    "\n",
    "print(\"误差值 = \" + str(preds[0]))\n",
    "print(\"准确率 = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tf1]",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
